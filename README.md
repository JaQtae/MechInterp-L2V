# MSc. Human-Centered Artificial Intelligence, DTU Compute anno 2025

This repository collects papers and resources related to applying the SAE interpretability idea to the Life2Vec model, scrutinizing its inner workings and assessing whether they are indeed interpretable.

---

## Primary Papers

The following papers are the primary ones contributing to the thesis and its foundational principles:

| Status       | Paper Title (Year)                                                                                                                                                                      | Description                                     | Link                                                                                                         |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ‚úîÔ∏è | **Anthropic: Towards Monosemanticity (2023)**                                                                                                                                           | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2023/monosemantic-features)                                           |
| ‚úîÔ∏è | **Anthropic: Scaling Monosemanticity (2024 follow-up)**                                                                                                                                 | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/scaling-monosemanticity/)                                         |
| ‚úîÔ∏è | **Life2Vec (Nature): Using sequences of life-events to predict human lives (2023)** <br> *First read-through (65%)*                                                                    | *(Key points summary‚Ä¶)*                         | [Link](https://www.nature.com/articles/s43588-023-00573-5)                                                     |
| üìñ  | **BERT ‚Äì Bidirectional Encoder Representation of Transformers (Google, 2018)**                                                                                                          | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/1810.04805)                                                                       |

---

## Resources

Materials related to Mechanistic Interpretability using Sparse Autoencoders on LLMs/Transformers. The checklist helps keeping track of progress  (üìñ indicates ‚Äúcurrently reading‚Äù). A brief description to capture the primary insights from each resource is available.

| Status       | Resource Title (Year/Info)                                                                                                                                                              | Description                                     | Link                                                                                                         |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ‚ùé  | **Anthropic: Update on SAE training regime (2024)**                                                                                                                                   | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/april-update/index.html#training-saes)                            |
| ‚úîÔ∏è | **Anthropic: Toy Models of Superposition (2022)**                                                                                                                                       | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2022/toy_model/index.html)                                             |
| ‚ùé  | **Sparse Crosscoders for Cross-Layer Features and Model Diffing (2024)**                                                                                                                | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/crosscoders/index.html)                                           |
| ‚úîÔ∏è | **Chris Olah ‚Äì Dark Matter of AI ‚Äì MechInterp (2024)**                                                                                                                                  | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/july-update/index.html#dark-matter)                               |
| üìñ  | **Privileged Bases in Transformer Residual Stream (2023)**                                                                                                                              | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2023/privileged-basis/index.html)                                      |
| ‚úîÔ∏è | **Chris Olah ‚Äì Zoom In: An Introduction to Circuits (2020)**                                                                                                                            | *(Key points summary‚Ä¶)*                         | [Link](https://distill.pub/2020/circuits/zoom-in/)                                                             |
| ‚úîÔ∏è | **A Mathematical Framework for Transformer Circuits (2021)**                                                                                                                            | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2021/framework/index.html)                                             |
| ‚ùé  | **The Geometry of Concepts: Sparse Autoencoder Feature Structure (2024)**                                                                                                                | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2410.19750)                                                                       |
| ‚ùé  | **Scaling and Evaluating SAE's (2024)**                                                                                                                                                 | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2406.04093)                                                                       |
| üìñ  | **Neel Nanda ‚Äì MechInterp (SAE) podcast (2024)**                                                                                                                                 | *(Key points summary‚Ä¶)*                         | [Link](https://podcasts.apple.com/dk/podcast/neel-nanda-mechanistic-interpretability-sparse-autoencoders/id1510472996?i=1000679600572) |
| ‚ùé  | **DeepMind ‚Äì Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 (2024)**                                                                                              | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2408.05147)                                                                       |
| ‚ùé  | **Trading off performance and human oversight in algorithmic policy: evidence from Danish college admissions (2024, Magnus)**                                                              | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2411.15348)                                                                       |
| üìñ  | **SAE's find highly interpretable features in LLM's (2023)**                                                                                                                            | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2309.08600)                                                                       |
| ‚ùé  | **(BERT?) Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors (2023)**                                           | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2103.15949)                                                                       |
| ‚ùé  | **Codebook features: Sparse and Discrete interpretability for neural networks (2023)**                                                                                                   | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2310.17230)                                                                       |
| ‚úîÔ∏è | **YouTube: 3Blue1Brown ‚Äì Visual explanation of LLM and Transformers (series)**                                                                                                          | *(Key points summary‚Ä¶)*                         | [Link](https://www.youtube.com/watch?v=wjZofJX0v4M)                                                            |
| ‚úîÔ∏è | **YouTube: Welch Labs ‚Äì Dark matter of AI**                                                                                                                                             | *(Key points summary‚Ä¶)*                         | [Link](https://www.youtube.com/watch?v=UGO_Ehywuxc)                                                             |
| ‚ùé  | **OpenAI: LLM can explain neurons in LLM (2023)**                                                                                                                                       | *(Key points summary‚Ä¶)*                         | [Link](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)                            |
| üìñ  | **Word2Vec ‚Äì Efficient Estimation of Word Representation in Vector Space (2013)**                                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/1301.3781)                                                                        |
| ‚ùé  | **DeepMind ‚Äì Improving Dictionary Learning with Gated SAE's (2024)**                                                                                                                     | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2404.16014)                                                                       |
| ‚ùé  | **Improving SAE's by SQRT-L1 and Removing lowest activating features (article, 2024)**                                                                                                    | *(Key points summary‚Ä¶)*                         | [Link](https://www.lesswrong.com/posts/YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest)     |
| ‚ùé  | **Scratchpads: Show your work ‚Äì Intermediate computation with LLM's (used in Anthropic) (2021)**                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2112.00114)                                                                       |
| üìñ  | **Neel Nhanda ‚Äì An opinionated list of favourite papers v2 (2024)**                                                                                                             | *(Key points summary‚Ä¶)*                         | [Link](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite) |
| ‚ùé  | **GitHub with MechInterp in LLM's ‚Äì Code and paper references**                                                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://github.com/ruizheliUOA/Awesome-Interpretability-in-Large-Language-Models)                         |
| ‚úîÔ∏è | **Decoding the Thought Vector (early work on sparsity) (2016)**                                                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://gabgoh.github.io/ThoughtVectors/)                                                               |
| ‚ùé  | **Multimodal neurons in ANN (fires regardless of modality) (2021)**                                                                                                                     | *(Key points summary‚Ä¶)*                         | [Link](https://distill.pub/2021/multimodal-neurons/)                                                          |
| ‚úîÔ∏è | **Comments and counter-points to Anthropic's paper**                                                                                                                                    | *(Key points summary‚Ä¶)*                         | [Link](https://www.lesswrong.com/posts/zzmhsKx5dBpChKhry/comments-on-anthropic-s-scaling-monosemanticity)       |
| ‚úîÔ∏è | **Interpretability Illusion for BERT (2021)** <br> *(First idea of dataset-level neuron levels; noted superposition effects)*                                                          | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2104.07143)                                                                       |
| ‚úîÔ∏è  | *(Return to this, original SAE idea?!)* **[Interim research report] Taking features out of superposition with sparse autoencoders (2022)**                                                                                        | *Highlights some challenges in terms of disentangling the complex neuron representations from the SAE. Uses MMCS, finds Goldilocks zone for L1 and dictionary size, important to track dead neurons and loss "stickiness" to identify optimal hyperparameters. (see Anthropic for more nuance?)*                         | [Link](https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition) |
| üìñ  | **Grammar of Life ‚Äì Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages (2025)**                                          | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2501.06346)                                                                       |

---

## Additional Buzzwords, Open Questions, and Explorations

- *k*-top SAE, Gated SAE, JumpReLU (w.r.t. the expensiveness of training SAE's; possibly also yielding better performance ‚Äî per Neel‚Äôs suggestions)
- GeLU vs. ReLU: Which activation function works best in practice?
- Measuring SAE performance: What are the most effective evaluation metrics?
- Investigate potential periodicity in Life2Vec embedding maps (e.g., via t-SNE) and any inherent structure.
- Explore causality and model introspection in Life2Vec, especially in relation to SAE usage.
- Understand Byte-paired tokenization: Find relevant papers and assess its impact.
- If Life2Vec employs BERT, explore the underlying paper for deeper insights.
- Delve into transformer circuits: How can QK and OV be ‚Äúdecomposed‚Äù and ‚Äúdisentangled‚Äù?
- **Open Question:** Does Life2Vec include an MLP? If not, how and where are the neuron activations obtained?

---

## SAE Resources

- [GitHub of SAE training code](https://github.com/ai-safety-foundation/sparse_autoencoder)
- [ARENA course and exercises (recommended by Neel)](https://arena-chapter0-fundamentals.streamlit.app/)
