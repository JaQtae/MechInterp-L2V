# MSc. Human-Centered Artificial Intelligence, DTU Compute anno 2025

This repository collects papers and resources related to applying the SAE interpretability idea to the Life2Vec model, scrutinizing its inner workings and assessing whether they are indeed interpretable.

---

## Primary Papers

The following papers are the primary ones contributing to the thesis and its foundational principles:

| Status       | Paper Title (Year)                                                                                                                                                                      | Description                                     | Link                                                                                                         |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ‚úîÔ∏è | **Anthropic: Towards Monosemanticity (2023)**                                                                                                                                           | *Uses L1 and L2 loss to promote sparsity in SAE. Good plots. Some critiques? Parallel to Cunningham et al., based on [this](https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition) partially. (More notes required)*                         | [Link](https://transformer-circuits.pub/2023/monosemantic-features)                                           |
| ‚úîÔ∏è | **Anthropic: Scaling Monosemanticity (2024 follow-up)**                                                                                                                                 | *Follow-up of above, focusing on the scaling of SAE's to the larger LLM's.*                         | [Link](https://transformer-circuits.pub/2024/scaling-monosemanticity/)                                         |
| ‚úîÔ∏è | **Life2Vec (Nature): Using sequences of life-events to predict human lives (2023)** <br> *First read-through (65%)*                                                                    | *Represents life-events as a sequence and trains on registry data using a BERT model. More models exist now (HGMAE, GPT-like soon‚Ñ¢Ô∏è), SOTA baseline.*                         | [Link](https://www.nature.com/articles/s43588-023-00573-5)                                                     |
| üìñ  | **BERT ‚Äì Bidirectional Encoder Representation of Transformers (Google, 2018)**                                                                                                          | *Contextualizes attention bidirectionally through masking. Transformer structure.*                         | [Link](https://arxiv.org/pdf/1810.04805)                                                                       |

---

## Resources

Materials related to Mechanistic Interpretability using Sparse Autoencoders on LLMs/Transformers. The checklist helps keeping track of progress  (üìñ indicates ‚Äúcurrently reading‚Äù). A brief description to capture the primary insights from each resource is available.

| Status       | Resource Title (Year/Info)                                                                                                                                                              | Description                                     | Link                                                                                                         |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| ‚ùé  | **Anthropic: Update on SAE training regime (2024)**                                                                                                                                   | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/april-update/index.html#training-saes)                            |
| ‚úîÔ∏è | **Anthropic: Toy Models of Superposition (2022)**                                                                                                                                       | *Investigates superposition principle in Transformers using small toy models. Very informative w.r.t. bases and relates back to several papers below.*                         | [Link](https://transformer-circuits.pub/2022/toy_model/index.html)                                             |
| ‚ùé  | **Sparse Crosscoders for Cross-Layer Features and Model Diffing (2024)**                                                                                                                | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/crosscoders/index.html)                                           |
| ‚úîÔ∏è | **Chris Olah ‚Äì Dark Matter of AI ‚Äì MechInterp (2024)**                                                                                                                                  | *(Key points summary‚Ä¶)*                         | [Link](https://transformer-circuits.pub/2024/july-update/index.html#dark-matter)                               |
| ‚úîÔ∏è  | **Privileged Bases in Transformer Residual Stream (2023)**                                                                                                                              | *Identifies ADAM as possible culprit of privileged bases - they observe heavy-tailed activations inside the computation basis, but not in the residual stream.*                         | [Link](https://transformer-circuits.pub/2023/privileged-basis/index.html)                                      |
| ‚úîÔ∏è | **Chris Olah ‚Äì Zoom In: An Introduction to Circuits (2020)**                                                                                                                            | *First notion of circuits. Visual explanation based on InceptionV1 (iirc), good grokking paper.*                         | [Link](https://distill.pub/2020/circuits/zoom-in/)                                                             |
| ‚úîÔ∏è | **A Mathematical Framework for Transformer Circuits (2021)**                                                                                                                            | *Foundational paper describing a Transformer as a circuit (QK, OV) and a mathematical way of understanding said circuits. Good entry for understanding foundational principles.*                         | [Link](https://transformer-circuits.pub/2021/framework/index.html)                                             |
| ‚ùé  | **The Geometry of Concepts: Sparse Autoencoder Feature Structure (2024)**                                                                                                                | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2410.19750)                                                                       |
| ‚ùé  | **Scaling and Evaluating SAE's (2024)**                                                                                                                                                 | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2406.04093)                                                                       |
| üìñ  | **Neel Nanda ‚Äì MechInterp (SAE) podcast (2024)**                                                                                                                                 | *General rant about open problems and interesting avenues/ways to think about mechanistic interpretability in a fast and growing field of research.*                         | [Link](https://podcasts.apple.com/dk/podcast/neel-nanda-mechanistic-interpretability-sparse-autoencoders/id1510472996?i=1000679600572) |
| üìñ  | **DeepMind ‚Äì Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2 (2024)**                                                                                              | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2408.05147)                                                                       |
| ‚ùé  | **Trading off performance and human oversight in algorithmic policy: evidence from Danish college admissions (2024, Magnus)**                                                              | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/abs/2411.15348)                                                                       |
| ‚úîÔ∏è  | **SAE's find highly interpretable features in LLM's (2023)**                                                                                                                            | *Uses autointerpretability (Bills et al.) with an SAE to intervene on activations layers in Pythia model. Comparison baselines are (1) default (2) random directions (3) PCA (4) ICA. Later layers less interpretable? Uses activation patching and ACDC for causality checks.*                         | [Link](https://arxiv.org/pdf/2309.08600)                                                                       |
| ‚ùé  | **(BERT?) Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors (2023)**                                           | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2103.15949)                                                                       |
| ‚ùé  | **Codebook features: Sparse and Discrete interpretability for neural networks (2023)**                                                                                                   | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2310.17230)                                                                       |
| ‚úîÔ∏è | **YouTube: 3Blue1Brown ‚Äì Visual explanation of LLM and Transformers (series)**                                                                                                          | *(Key points summary‚Ä¶)*                         | [Link](https://www.youtube.com/watch?v=wjZofJX0v4M)                                                            |
| ‚úîÔ∏è | **YouTube: Welch Labs ‚Äì Dark matter of AI**                                                                                                                                             | *(Key points summary‚Ä¶)*                         | [Link](https://www.youtube.com/watch?v=UGO_Ehywuxc)                                                             |
| ‚úîÔ∏è*  | **OpenAI: LLM can explain neurons in LLM (2023)**                                                                                                                                       | *Pretty sure this is AutoInterp or early version thereof. Probably not useful with our data. Explains correlations, not mechanisms.*                         | [Link](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)                            |
| üìñ  | **Word2Vec ‚Äì Efficient Estimation of Word Representation in Vector Space (2013)**                                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/1301.3781)                                                                        |
| ‚úîÔ∏è  | **Google DeepMind ‚Äì Improving Dictionary Learning with Gated SAE's (2024)**                                                                                                                     | *Combats bias in loss from L1 by harming L2. There's some valuable comments about the feature shrinkage phenomenon, which Gated combats [here](https://www.lesswrong.com/posts/vdoiWXeouGsZEYgrg/improving-dictionary-learning-with-gated-sparse-autoencoders). Neel's team's preferred way of training SAEs. Makes a "gate", s.t. JumpReLU behaviour on preactivation. Numerous other important definitions!!*                         | [Link](https://arxiv.org/pdf/2404.16014)                                                                       |
| ‚ùé  | **Improving SAE's by SQRT-L1 and Removing lowest activating features (article, 2024)**                                                                                                    | *(Key points summary‚Ä¶)*                         | [Link](https://www.lesswrong.com/posts/YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest)     |
| ‚ùé  | **Scratchpads: Show your work ‚Äì Intermediate computation with LLM's (used in Anthropic) (2021)**                                                                                         | *(Key points summary‚Ä¶)*                         | [Link](https://arxiv.org/pdf/2112.00114)                                                                       |
| üìñ  | **Neel Nhanda ‚Äì An opinionated list of favourite papers v2 (2024)**                                                                                                             | *Contains primary papers for new people in the field and highlights important sections or ideas of each paper to focus on.*                         | [Link](https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite) |
| ‚ùé  | **GitHub with MechInterp in LLM's ‚Äì Code and paper references**                                                                                                                         | *A general repo I found.*                         | [Link](https://github.com/ruizheliUOA/Awesome-Interpretability-in-Large-Language-Models)                         |
| ‚úîÔ∏è | **Decoding the Thought Vector (early work on sparsity) (2016)**                                                                                                                         | *Early work on sparsity and to some extent superposition (iirc). Primarily through the lens of visual model.*                         | [Link](https://gabgoh.github.io/ThoughtVectors/)                                                               |
| ‚ùé  | **Multimodal neurons in ANN (fires regardless of modality) (2021)**                                                                                                                     | *Despite modality a concept can be recognized in neuron activations.*                         | [Link](https://distill.pub/2021/multimodal-neurons/)                                                          |
| ‚úîÔ∏è | **Comments and counter-points to Anthropic's "Scaling" paper**                                                                                                                                    | *Unclear if dictionary learning can find specific features of interest. Correlations between neurons and features seems high, despite Anthropic's claim? Mostly on limitations or claims made that might be deceiving.*                         | [Link](https://www.lesswrong.com/posts/zzmhsKx5dBpChKhry/comments-on-anthropic-s-scaling-monosemanticity)       |
| ‚úîÔ∏è | **Interpretability Illusion for BERT (2021)**                                                          | *Strong evidence for superposition and illusions of concepts across datasets (intuitively makes sense imo). BERT activation space orgnanizes similar sentences in clusters, but linear directions in activation space do not correspond to linguistic concepts. Early work.*                         | [Link](https://arxiv.org/pdf/2104.07143)                                                                       |
| ‚úîÔ∏è  | *(Return to this, original SAE idea?!)* **[Interim research report] Taking features out of superposition with sparse autoencoders (2022)**                                                                                        | *Highlights some challenges in terms of disentangling the complex neuron representations from the SAE. Uses MMCS, finds Goldilocks zone for L1 and dictionary size, important to track dead neurons and loss "stickiness" to identify optimal hyperparameters. (see Anthropic for more nuance?)*                         | [Link](https://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition) |
| ‚úîÔ∏è  | **Grammar of Life ‚Äì Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages (2025)**                                          | *Features capture generalizable abstractions in language. Uses Gated SAEs to reduce bias from L1 term (unlike Anthropic), which "separates <ins> selecting features to use </ins> and <ins> estimating activation magnitude of those directions </ins>". Attribution patching for causally relevant features. Quality of SAEs by measuring proportion of loss recovered when replacing model activations with SAE reconstructions thereof.*                         | [Link](https://arxiv.org/pdf/2501.06346)                                                                       |
| ‚ùé  | **Tom McGrath - SAE takes w.r.t. AutoInterp (Robert Huben's pointer).**                                          | *Auto-Interp has its limitations, such as scoring really well on "trivial features", but can be a useful tool in attempting to interpret each feature.*                         | [Link](https://x.com/banburismus_/status/1886128587687764402)                                                                       |
| ‚ùé  | **[Research Report] SAE find only 9/180 (33/180) board state features in OthelloGPT**                                          | *SAE might not find a significant number of features from the LM, in this case corresponding to human-understandable ontology of these features (meaning we can categorize and label it). So, they show despite having features we understand, the SAE might not uncover all of them.*                         | [Link](https://www.lesswrong.com/posts/BduCMgmjJnCtc7jKc/research-report-sparse-autoencoders-find-only-9-180-board)     
| ‚úîÔ∏è  | **Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders**                                          | *Has a positive threshold which indicates when a feature is considered active (and separates it from its magnitude). Alternative to Gated SAE's (Aug vs. Apr), similar interpretability but slightly better reconstruction (at fixed sparsity). Bypasses L0-like loss (L1) and directly uses L0. Estimated via straight-through estimators (STE). Uses KDE to estimate expected probabilities w.r.t. L0 gradient, and they use pseudo-derivatives. Has higher frequency features (like TopK) compared to Gated SAE's, but considered mild improvement of prevailing methods due to no auxillary term.*                         | [Link](https://arxiv.org/pdf/2407.14435)     

---

## Additional Buzzwords, Open Questions, and Explorations

- *k*-top SAE, Gated SAE, JumpReLU (per Neel‚Äôs suggestions Gated is the way to go? See comments on papers. (Pros/Cons: Resampling, reconstruction vs sparsity, L0-like (L1) or L0 (pseudo-derivative))
  - Gated SAE's are better than what Anthropic did, because they avoid the bias introduced from L1 w.r.t. harming L2. (Rajamanoharan et al. (2024)) 
- GeLU vs. ReLU: Which activation function works best in practice? (Neel says GeLU...)
- Measuring SAE performance: What are the most effective evaluation metrics? (Huben email sources, his input is to focus on how you can evaluate the features)
- Understand Byte-paired tokenization: Find relevant papers and assess its impact. (Used in tokenization?
- Delve into transformer circuits: How can QK and OV be ‚Äúdecomposed‚Äù and ‚Äúdisentangled‚Äù?
- **Open Question:** Does Life2Vec include an MLP? If not, how and where are the neuron activations obtained?
- Is autointerpretability possible? Is it good? Anyhow, how is it possible to see what maximally activates a specific feature based on input for L2V???
- [Huben] Check if your features serve as a classifier for an external set of interpretable concepts, see [his work](https://www.lesswrong.com/posts/BduCMgmjJnCtc7jKc/research-report-sparse-autoencoders-find-only-9-180-board), [Karvonen et al.](https://arxiv.org/abs/2408.00113), and [Gao et al., section 4.2](https://arxiv.org/pdf/2406.04093)
- [Huben] Find a way to represent your features so that their usefulness is immediately visible, see e.g. [his attempt](https://www.lesswrong.com/posts/f7gd7riceJPaPKkNS/saes-you-can-see-applying-sparse-autoencoders-to-clustering) or [Anthropic's graphs](https://transformer-circuits.pub/2024/scaling-monosemanticity/#:~:text=highly%20consistent%20with%20the%20proposed%20interpretation.).

---

## SAE Resources

- [GitHub of SAE training code](https://github.com/ai-safety-foundation/sparse_autoencoder)
- [ARENA course and exercises (recommended by Neel)](https://arena-chapter0-fundamentals.streamlit.app/)
